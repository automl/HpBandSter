{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing hyperband-optimization: Connecting HpBandSter and CAVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we present an example on how to connect our tools <a href=\"https://github.com/automl/CAVE\" target=\"_blank\">CAVE</a> and <a href=\"https://github.com/automl/HpBandSter\" target=\"_blank\">HpBandSter</a> to efficiently optimize a neural network and subsequently analyze and visualize the optimization pr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the frameworks\n",
    "\n",
    "### HpBandSter\n",
    "\n",
    "<a href=\"https://github.com/automl/HpBandSter\" target=\"_blank\">HpBandSter</a> (HyperBand on Steroids) implements recently published methods for optimizing hyperparameters of machine learning algorithms. One of the implemented algorithms is <a href=\"https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html\" target=\"_blank\">BOHB</a>, combining Bayesian Optimization and HyperBand to efficiently search for well performing configurations. Learn more about this method by reading our paper, published at <a href=\"http://proceedings.mlr.press/v80/falkner18a.html\" target=\"_blank\">ICML 2018</a>.\n",
    "\n",
    "### CAVE\n",
    "\n",
    "<a href=\"https://github.com/automl/CAVE\" target=\"_blank\">CAVE</a> (Configuration Assessment, Visualization and Evaluation) is designed to create comprehensive reports about an optimization process. The resulting figures and interactive plots can be used to gain insights in the parameter importance, feature importance, search behaviour and quality.\n",
    "\n",
    "### Installation requirements\n",
    "\n",
    "You need to install HpBandSter and CAVE to run this example. Both can be installed using pip (i.e. `pip install cave hpbandster`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BOHB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We deactivate logging to ensure readability\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "# Also, we suppress warnings.\n",
    "# If there are problems for you executing this notebook, you might want to comment this out.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Preparing BOHB\n",
    "*Worker*: We need a <a href=\"https://automl.github.io/HpBandSter/build/html/core/worker.html\" target=\"_blank\">Worker</a> to define the kind of computation we want to optimize. An exemplary worker computing the validation loss of a configurable network on the digits dataset can be found under `hpbandster/examples/example_connect_cave_and_bohb.py`.\n",
    "\n",
    "*ConfigSpace*: Every problem needs a description of the search space to be complete. In HpBandSter, a ConfigurationSpace-object defines all hyperparameters, their ranges and dependencies between them.\n",
    "\n",
    "In our example here, the search space consists of the hyperparameters:\n",
    "\n",
    "|         Name        |     Type    |      Values      |     Condition    |\n",
    "|:-------------------:|:-----------:|:----------------:|:----------------:|\n",
    "| activation-function | categorical | {'relu', 'tanh'} |                  |\n",
    "|    learning-rate    |    float    |   [1e-6 - 1e-2]  |                  |\n",
    "|        solver       | categorical |  {'sgd', 'adam'} |                  |\n",
    "|        beta_1       |    float    |      [0, 1]      | solver == 'adam' |\n",
    "|        beta_2       |    float    |      [0, 1]      | solver == 'adam' |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from example_connect_cave_and_bohb import MyWorker, get_configspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 1.3) Setting up the HpBandSter Nameserver and starting the optimization run\n",
    "\n",
    "**NOTE:** Unfortunately, the configuration space is *not saved automatically* to file but this step is mandatory for the analysis with CAVE.  \n",
    "We recommend to save the configuration space every time you use BOHB.\n",
    "We do this by using the ConfigSpace-to-json-writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from ConfigSpace.read_and_write import pcs_new\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "\n",
    "# Create and save a configuration space\n",
    "config_space = get_configspace()\n",
    "out_dir = 'example_cave_and_bohb_in_jupyter'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(out_dir, 'configspace.pcs'), 'w') as fh:\n",
    "    fh.write(pcs_new.write(config_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - initiating communication, creating a nameserver:\n",
    "The <a href=\"https://automl.github.io/HpBandSter/build/html/core/nameserver.html\" target=\"_blank\">nameserver</a> is a small service that keeps track of all running processes and their IP addresses and ports. It could be a 'static' server with a permanent address, but here it will be started for the local machine with a random port. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '0'  # Every run has to have a unique (at runtime) id.\n",
    "NS = hpns.NameServer(run_id=run_id, host='localhost', port=0)\n",
    "ns_host, ns_port = NS.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - creating the worker: \n",
    "The worker implements the actual problem that is optimized. Its 'compute'-method will be called later by the BOHB-optimizer repeatedly with the sampled configurations and return the computed loss (and additional information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MyWorker(nameserver=ns_host,\n",
    "             nameserver_port=ns_port,\n",
    "             run_id=run_id,  # same as nameserver's\n",
    "            )\n",
    "w.run(background=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:  \n",
    "Create an optimizer object, which samples configurations from the ConfigurationSpace, using succesive halving to assign budgets for execution. Further information on what qualifies as a budget <a href=\"https://automl.github.io/HpBandSter/build/html/quickstart.html#meaningful-budgets-and-number-of-iterations\" target=\"_blank\">can be found in the documentation.</a>\n",
    "\n",
    "**NOTE:** BOHB does not build a new model at the beginning of every SuccessiveHalving run. Instead it collects all evaluations on all budgets and uses the largest budget with enough evaluations as a base for future evaluations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Log the optimization results for later analysis\n",
    "result_logger = hpres.json_result_logger(directory=out_dir, overwrite=True)\n",
    "\n",
    "bohb = BOHB(  configspace=config_space,\n",
    "              run_id=run_id,  # same as nameserver's\n",
    "              eta=2, min_budget=5, max_budget=100,  # Hyperband parameters\n",
    "              nameserver=ns_host, nameserver_port=ns_port,\n",
    "              result_logger=result_logger,\n",
    "           )\n",
    "\n",
    "# Then start the optimizer. The n_iterations parameter specifies\n",
    "# the number of iterations to be performed in this run\n",
    "res = bohb.run(n_iterations=2)\n",
    "\n",
    "# After the run is finished, the services started above need to be shutdown.\n",
    "# This ensures that the worker, the nameserver and the master all properly exit\n",
    "# and no (daemon) threads keep running afterwards.\n",
    "# In particular we shutdown the optimizer (which shuts down all workers) and the nameserver.\n",
    "bohb.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://automl.github.io/HpBandSter/build/html/core/result.html\" target=\"_blank\">Result-object</a> offers access to basic statistics, as well as the best configuration (incumbent) and the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The returned result object holds informations about the optimization run\n",
    "# like the incumbent (=best) configuration.\n",
    "id2config = res.get_id2config_mapping()\n",
    "print('A total of %i unique configurations where sampled.' % len(id2config.keys()))\n",
    "print('A total of %i runs where executed.' % len(res.get_all_runs()))\n",
    "print('Best configuration found: {}'.format(id2config[res.get_incumbent_id()]['config']))\n",
    "\n",
    "# The incumbent trajectory is a dictionary with all the configuration IDs, the times the runs\n",
    "# finished, their respective budgets, and corresponding losses.\n",
    "# It's used to do meaningful plots of the optimization process.\n",
    "incumbent_trajectory = res.get_incumbent_trajectory()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(incumbent_trajectory['times_finished'], incumbent_trajectory['losses'])\n",
    "plt.xlabel('wall clock time [s]')\n",
    "plt.ylabel('incumbent loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the results in CAVE\n",
    "\n",
    "### 2.1) Creating a HTML-report with CAVE\n",
    "\n",
    "Creating the report with CAVE is very straight-forward. Simply provide the output-directory of the BOHB-analysis in CAVE's `--folders` argument and specify `--file_format` as `BOHB`. You can do this by commandline ('!' simply executes the command as if it was executed on the command line):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cave --folders workflow_result --file_format BOHB --output CAVE-workflow-result --verbose_level OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After CAVE finished the report, you can have a look at it with your favorite browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! firefox CAVE-workflow-result/report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Using CAVE from within Python\n",
    "\n",
    "Of course you can use CAVE on a module-level. Import and instantiate it (very similarily to the commandline). By default, CAVE even outputs all analysis results in a jupyter-cell-compatible way. Of course, the HTML-report is built meanwhile, so you don't have to run time-consuming analyzing-methods repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cave.cavefacade import CAVE\n",
    "\n",
    "cave = CAVE(folders=[\"workflow_result\"],\n",
    "            output_dir=\"test_jupyter\",\n",
    "            ta_exec_dir=[\".\"],\n",
    "            file_format='BOHB',\n",
    "            verbose_level='WARNING'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting plot for BOHB might be a visualization of the learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.bohb_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the individual budgets via the 'run'-keyword-argument of each analysis-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.print_budgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.overview_table(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each budget, we can compare the default against the incumbent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.compare_default_incumbent(run='budget_25.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.performance_table(run='budget_25.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parameter-importance analysis, CAVE uses <a href=\"https://github.com/automl/ParameterImportance\" target=\"_blank\">PIMP</a> , a package that provides multiple approaches to parameter-importance analysis. We can easily invoke them via CAVE, of course. To estimate the importance, random forests are used to predict performances of configurations that were not executed. This is difficult for big budgets with few configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.cave_fanova(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.cave_ablation(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.local_parameter_importance(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each budget, we can compare the different parameter-importance-methods that have already been run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.pimp_comparison_table(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze BOHB's behaviour, we can check out the configurator footprint, cost-over-time and parallel coordinated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.configurator_footprint(run='budget_12.5', time_slider=True, num_quantiles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.cost_over_time(run=\"budget_25.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.parallel_coordinates(run='budget_12.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
